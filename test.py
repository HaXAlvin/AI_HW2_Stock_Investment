# -*- coding: utf-8 -*-
"""CE6020_hw2_code_test_example.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZhWZkx5N77QWrfZ2MUa4s9Wt9dIFPFGC
"""

import gym
import gym_anytrading
from gym_anytrading.envs import StocksEnv

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical

import os
import argparse
import pandas as pd
import json

"""輸入參數設計 (不可動)
"""
parser = argparse.ArgumentParser()
parser.add_argument('--start', '-s', type=int, default=10,
                    help='stock investment start date index')
parser.add_argument('--end', '-e', type=int, default=300,
                    help='stock investment end date index')
parser.add_argument('--output_action_file_pathname', '-f', type=str, default='action.txt',
                    help='output action file pathname')

args = parser.parse_args()

predict_start_date_index = args.start
predict_end_date_index = args.end
output_action_file_pathname = args.output_action_file_pathname

""" 以下參數之設定僅提供 ipynb 檔案測試用 (不得用於繳交之 test.py 檔案)
predict_start_date_index = 10, predict_end_date_index = 300, output_action_file_pathname = 'action.txt'
"""
# predict_start_date_index = 10
# predict_end_date_index = 300
# output_action_file_pathname = 'action.txt'

STOCKS_TSMC = pd.read_csv('./2330_stock.csv')

"""模型設計 (可動)
network(PolicyGradientNetwork), optimizer 設計可依照同學設計做修改
"""
# 模型記錄點檔案位置
checkpoint_file_path = './model.ckpt'

# 設定 seed
# torch.manual_seed(1234)
# np.random.seed(1234)

# 請把您的模型複製過來


class Actor(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=32):
        super(Actor, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, action_dim)

    def forward(self, x): # input state, output prob of action
        x = self.fc1(x)
        x = torch.tanh(x)
        x = self.fc2(x)
        x = torch.softmax(x, dim=-1)
        return x

class Critic(nn.Module):
    def __init__(self, state_dim, hidden_dim=32):
        super(Critic, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, 1)

    def forward(self, x): # input state, output score
        x = self.fc1(x)
        x = torch.relu(x)
        x = self.fc2(x)
        return x

class ACAgent():
    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.9):
        self.actor = Actor(state_dim, action_dim)
        self.critic = Critic(state_dim)
        self.opt = optim.Adam([{"params":self.actor.parameters()}, {"params":self.critic.parameters()}],lr=lr)
        self.gamma = gamma
        # records
        self.log_prob = []
        self.score = []
        self.reward = []

    def recode(self, log_prob:torch.Tensor, score:torch.Tensor, reward:torch.Tensor):
        self.log_prob.append(log_prob)
        self.score.append(score)
        self.reward.append(reward)

    def sample_action(self, state):
        action_prob = self.actor(state)
        action_dist = Categorical(action_prob)
        action = action_dist.sample()
        log_prob = action_dist.log_prob(action)
        return action.detach().item(), log_prob

    def evaluate_score(self, state):
        return self.critic(state)[0]

    def learn(self):
        # calculating discounted rewards
        rewards = []
        dis_reward = 0
        for reward in self.reward[::-1]:
            dis_reward = reward + self.gamma * dis_reward
            rewards.insert(0, dis_reward) 
        # normalizing the rewards
        rewards = torch.tensor(rewards)
        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-9)
        # counting the loss
        loss = 0
        for log_prob, score, reward in zip(self.log_prob, self.score, rewards):
            advantage = reward - score.item()
            action_loss = -log_prob * advantage
            value_loss = F.smooth_l1_loss(score, reward)
            loss += (action_loss + value_loss)
        self.opt.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_value_(list(self.actor.parameters())+list(self.critic.parameters()), 0.5)
        self.opt.step()

    def clear_memory(self):
        del self.log_prob[:]
        del self.score[:]
        del self.reward[:]

    def load_ckpt(self, ckpt_path):
        if os.path.exists(ckpt_path):
            checkpoint = torch.load(ckpt_path)
            self.actor.load_state_dict(checkpoint['actor_state_dict'])
            self.critic.load_state_dict(checkpoint['critic_state_dict'])
            self.opt.load_state_dict(checkpoint['optimizer_state_dict'])
        else:
            print("Checkpoint file not found, use default settings")

    def save_ckpt(self, ckpt_path):
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'optimizer_state_dict': self.opt.state_dict(),
        }, ckpt_path)



"""模型參數設計 (可動)
使用之資料欄位可依照同學設計做修改
"""


def my_process_data(env):
    start = env.frame_bound[0] - env.window_size
    end = env.frame_bound[1]
    prices = env.df.loc[:, 'Low'].to_numpy()[start:end]
    # 這邊可自訂想要使用的 feature
    signal_features = env.df.loc[:, ['Close', 'Open']].to_numpy()[start:end]
    return prices, signal_features


class MyStocksEnv(StocksEnv):
    _process_data = my_process_data


# window_size: 能夠看到幾天的資料當作輸入, frame_bound: 想要使用的資料區間
env = MyStocksEnv(df=STOCKS_TSMC, window_size=10, frame_bound=(
    predict_start_date_index, predict_end_date_index))

"""測試 Agent 模型預測結果"""

test_agent = ACAgent(env.shape[0] * env.shape[1], 2)

test_agent.load_ckpt(checkpoint_file_path)
test_agent.actor.eval()  # 測試前先將 network 切換為 evaluation 模式
test_agent.critic.eval()  # 測試前先將 network 切換為 evaluation 模式

state = env.reset()

actions_list = []

while True:
    state = torch.FloatTensor(state).flatten()
    action, _ = test_agent.sample_action(state)
    state, reward, done, info = env.step(action)
    actions_list.append(action)

    if done:
        break

# 輸出檔案內容必須包含以下五個資料
output_content = {
    'start': predict_start_date_index,
    'end': predict_end_date_index,
    'total_reward': info['total_reward'],
    'total_profit': info['total_profit'],
    'action': actions_list
}

# 輸出模型動作選擇文件
with open(output_action_file_pathname, 'w+') as fs:
    json.dump(output_content, fs)

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code isn't done, half finish to try actor-critic structure in this way:\n",
        "https://cloud.tencent.com/developer/article/1635793"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPYuGj8UbFxl"
      },
      "source": [
        "## 環境初始化\n",
        "新增 [gym-anytrading](https://github.com/AminHP/gym-anytrading) 套件，當作本次任務的環境。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaKQ5WnAbXfY"
      },
      "source": [
        "引入需要的套件，並設定隨機參數種子。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3jqnksiif7p"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.distributions import Categorical\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from gym_anytrading.envs import StocksEnv\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "# 設定 seed\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1osFmodMX2S_"
      },
      "source": [
        "## 使用自訂資料集\n",
        "本次作業同樣使用台積電 (2330) 2010/1月~2022/10月資料作為使用的內容。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "dUEELNVptuaK",
        "outputId": "7ea53f34-1a00-4307-8680-72722d9e2708"
      },
      "outputs": [],
      "source": [
        "STOCKS_TSMC = pd.read_csv('./CE6020_hw2_resource/2330_stock.csv')\n",
        "STOCKS_TSMC\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNHqKFBlb4tt"
      },
      "source": [
        "將台積電資料輸入股票環境，並設定本次環境範圍與輸入天數資料 (欄位內容、天數等)。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsxfPOwWuoCm"
      },
      "outputs": [],
      "source": [
        "def my_process_data(env):\n",
        "    start = env.frame_bound[0] - env.window_size\n",
        "    end = env.frame_bound[1]\n",
        "    prices = env.df.loc[:, 'Low'].to_numpy()[start:end]\n",
        "    # 這邊可修改想要使用的 feature\n",
        "    signal_features = env.df.loc[:, ['Close', 'Open']].to_numpy()[start:end]\n",
        "    return prices, signal_features\n",
        "\n",
        "\n",
        "class MyStocksEnv(StocksEnv):\n",
        "    # 除 _process_data 外，其餘功能 (class function) 禁止覆寫\n",
        "    _process_data = my_process_data\n",
        "\n",
        "\n",
        "# window_size: 能夠看到幾天的資料當作輸入, frame_bound: 想要使用的資料日期區間\n",
        "# 可修改 frame_bound 來學習不同的環境資料\n",
        "# 不可修改此處 window_size 參數\n",
        "env = MyStocksEnv(df=STOCKS_TSMC, window_size=10, frame_bound=(1000, 1500))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m3sc7Q5ct1K"
      },
      "source": [
        "檢視環境參數"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "wQLiS588uSNp",
        "outputId": "cf9274b2-3fca-4f3f-fbc3-683e5d858861"
      },
      "outputs": [],
      "source": [
        "print(\"env information:\")\n",
        "print(\"> shape:\", env.shape)\n",
        "print(\"> df.shape:\", env.df.shape)\n",
        "print(\"> prices.shape:\", env.prices.shape)\n",
        "print(\"> signal_features.shape:\", env.signal_features.shape)\n",
        "print(\"> max_possible_profit:\", env.max_possible_profit())\n",
        "\n",
        "env.reset()\n",
        "env.render()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "anTq_5PrTCYG",
        "outputId": "2c17df52-d8ee-4624-920b-60e4d2edc04e"
      },
      "outputs": [],
      "source": [
        "observation = env.reset()\n",
        "while True:\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    # ob=10d stock * 2 features\n",
        "    # re = earn\n",
        "    # done = Bool\n",
        "    # info = reward & profit & position\n",
        "    if done:\n",
        "        print(info)\n",
        "        break\n",
        "plt.cla()\n",
        "env.render_all()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5paWqo7tWL2"
      },
      "source": [
        "## Actor-Critic\n",
        "\n",
        "\n",
        "輸入是 20-dim (10天*2欄位)，輸出則是離散的二個動作之一 (賣=0 或 買=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-iksrp9yl10"
      },
      "outputs": [],
      "source": [
        "from torch.distributions import MultivariateNormal\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=16):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, action_dim)\n",
        "\n",
        "    def forward(self, x): # input state, output prob of action\n",
        "        x = self.fc1(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.fc2(x)\n",
        "        x = torch.softmax(x, dim=-1)\n",
        "        return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_dim, hidden_dim=16):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x): # input state, output score\n",
        "        x = self.fc1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class ACAgent():\n",
        "\n",
        "    def __init__(self, state_dim, action_dim,lr=0.001,gamma=0.5):\n",
        "        self.actor = Actor(state_dim, action_dim)\n",
        "        self.critic = Critic(state_dim)\n",
        "        self.actor_opt = optim.RMSprop(self.actor.parameters(), lr=lr)\n",
        "        self.critic_opt = optim.RMSprop(self.critic.parameters(), lr=lr)\n",
        "        # self.action_log_prob = None\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def sample_action(self, state):\n",
        "        action_prob = self.actor(state)\n",
        "        action_dist = Categorical(action_prob)\n",
        "        action = action_dist.sample()\n",
        "        log_prob = action_dist.log_prob(action)\n",
        "        return action.detach().item(), log_prob\n",
        "\n",
        "    def evaluate_score(self,state):\n",
        "        return self.critic(state)\n",
        "\n",
        "    def actor_learn(self, td_error, log_prob):\n",
        "        loss = torch.mean(log_prob*td_error)\n",
        "        self.actor_opt.zero_grad()\n",
        "        loss.backward()\n",
        "        self.actor_opt.step()\n",
        "        return loss\n",
        "    \n",
        "    def critic_learn(self, reward, state, next_state, done):\n",
        "        # done = 0 if done else 1\n",
        "        # done = 1\n",
        "        next_score = self.evaluate_score(next_state)\n",
        "        score = self.evaluate_score(state)\n",
        "        loss = reward + done*self.gamma*next_score - score\n",
        "        loss = torch.square(loss)\n",
        "        td_error = loss.detach()\n",
        "        self.critic_opt.zero_grad()\n",
        "        loss.backward()\n",
        "        self.critic_opt.step()\n",
        "        return td_error\n",
        "\n",
        "    def load_ckpt(self, ckpt_path):\n",
        "        if os.path.exists(ckpt_path):\n",
        "            checkpoint = torch.load(ckpt_path)\n",
        "            self.actor.load_state_dict(checkpoint['actor_state_dict'])\n",
        "            self.critic.load_state_dict(checkpoint['critic_state_dict'])\n",
        "            self.actor_opt.load_state_dict(checkpoint['actor_optimizer_state_dict'])\n",
        "            self.critic_opt.load_state_dict(checkpoint['critic_optimizer_state_dict'])\n",
        "        else:\n",
        "            print(\"Checkpoint file not found, use default settings\")\n",
        "\n",
        "    def save_ckpt(self, ckpt_path):\n",
        "        torch.save({\n",
        "            'actor_state_dict': self.actor.state_dict(),\n",
        "            'critic_state_dict': self.critic.state_dict(),\n",
        "            'actor_optimizer_state_dict': self.actor_opt.state_dict(),\n",
        "            'critic_optimizer_state_dict': self.critic_opt.state_dict(),\n",
        "        }, ckpt_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynbqJrhIFTC3"
      },
      "source": [
        "再來，搭建一個簡單的 agent，並搭配上方的 policy network 來採取行動。\n",
        "這個 agent 能做到以下幾件事：\n",
        "- `learn()`：從記下來的 log probabilities 及 rewards 來更新 policy network。\n",
        "- `sample()`：從 environment 得到 observation 之後，利用 policy network 得出應該採取的行動。\n",
        "而此函式除了回傳抽樣出來的 action，也會回傳此次抽樣的 log probabilities。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehPlnTKyRZf9"
      },
      "source": [
        "最後，建立一個 network 和 agent，就可以開始進行訓練了。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfJIvML-RYjL"
      },
      "outputs": [],
      "source": [
        "agent = ACAgent(env.shape[0]*env.shape[1], 2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouv23glgf5Qt"
      },
      "source": [
        "## 訓練 Agent\n",
        "\n",
        "現在我們開始訓練 agent。\n",
        "透過讓 agent 和 environment 互動，我們記住每一組對應的 log probabilities 及 reward，並在資料日期區間結束後，回放這些「記憶」來訓練 policy network。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77,
          "referenced_widgets": [
            "da5a2579b8784924b4733cc578d2a309",
            "752a369955b54de9b714a2e54ea33824",
            "64cb3c25c9a548789b78192cc5e8f214",
            "abe58da207ce4efc9649a8e7fc717682",
            "b1723c3ecd394c4ebca7ab2063af2d89",
            "28658a1c9e724b9a85cc2878faf2d8e2",
            "017bb6875a794fb5be36a5edc9edb03d",
            "ee8a2b45b6ba48db9ccdd742c0e74fda",
            "a2e05b9960dc4800974704b05adf997e",
            "b1ec572744fa41379b26e872c4c9631a",
            "4de76787a98948b89f9ee9b311a42253"
          ]
        },
        "id": "vg5rxBBaf38_",
        "outputId": "d6438c31-c8ee-4a2e-e412-9d2dede47877"
      },
      "outputs": [],
      "source": [
        "EPISODE_PER_BATCH = 5  # 每蒐集 5 個 episodes 更新一次 agent\n",
        "NUM_BATCH = 10     # 總共更新 400 次\n",
        "CHECKPOINT_PATH = './model.ckpt'  # agent model 儲存位置\n",
        "\n",
        "avg_total_rewards = []\n",
        "\n",
        "agent.actor.train()  # 訓練前，先確保 network 處在 training 模式\n",
        "agent.critic.train()\n",
        "\n",
        "prg_bar = tqdm(range(NUM_BATCH), miniters=1)\n",
        "for batch in prg_bar:\n",
        "    log_probs, rewards = [], []\n",
        "    total_rewards = []\n",
        "    # 蒐集訓練資料\n",
        "    for episode in range(EPISODE_PER_BATCH):\n",
        "        state = env.reset()\n",
        "        state = torch.FloatTensor(state).flatten()\n",
        "        # print(state)\n",
        "        records = {\"state\":[],\"next_state\":[],\"reward\":[],\"log_prob\":[]}\n",
        "        # episode_rewards = []\n",
        "        total_step = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action, log_prob = agent.sample_action(state)\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_state = torch.FloatTensor(next_state).flatten()\n",
        "            # print(reward)\n",
        "            # if reward > 0:\n",
        "            \n",
        "            records[\"reward\"].append(reward)\n",
        "            \n",
        "            # if len(set(records[\"reward\"][-5:])) == 1 and len(records[\"reward\"]) >=5:\n",
        "            #     records[\"reward\"][-1] -= 10\n",
        "            td_error = agent.critic_learn(reward,state, next_state)\n",
        "            # agent.actor_learn(td_error, log_prob)\n",
        "\n",
        "            # records[\"reward\"].append(reward)\n",
        "            # records[\"state\"].append(state)\n",
        "            # records[\"next_state\"].append(next_state)\n",
        "            # records[\"log_prob\"].append(log_prob)\n",
        "            state = next_state\n",
        "            total_step += 1\n",
        "        # td_errors = agent.critic_learn(records[\"reward\"],records[\"state\"], records[\"next_state\"])\n",
        "        # agent.actor_learn(td_errors, records[\"log_prob\"])\n",
        "        total_rewards.append(info['total_reward'])\n",
        "        # rewards.append(np.full(total_step, -10 if info['total_reward'] < 0 else 10)+np.array(ep_rewards))  # 設定同一個 episode 每個 action 的 reward 都是 total reward\n",
        "        rewards.append(np.full(total_step, info['total_reward']))  # 設定同一個 episode 每個 action 的 reward 都是 total reward\n",
        "        # env.render_all()\n",
        "        # plt.show()\n",
        "    # 紀錄訓練過程\n",
        "    avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
        "    avg_total_rewards.append(avg_total_reward)\n",
        "    prg_bar.set_description(f\"Average Reward: {avg_total_reward: 04.2f}, Final Reward: {info['total_reward']: 04.2f}, Final Profit: {info['total_profit']: 04.2f}\")\n",
        "\n",
        "    # 更新網路\n",
        "    rewards = np.concatenate(rewards, axis=0)\n",
        "    rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)  # 將 reward 正規標準化\n",
        "    # agent.learn(torch.stack(log_probs), torch.from_numpy(rewards))\n",
        "\n",
        "# 儲存 agent model 參數\n",
        "agent.save_ckpt(CHECKPOINT_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNb_tuFYhKVK"
      },
      "source": [
        "### 訓練結果\n",
        "\n",
        "訓練過程中，我們持續記下了 `avg_total_reward`，這個數值代表的是：每次更新 policy network 前，我們讓 agent 玩數個回合（episodes），而這些回合的平均 total rewards 為何。\n",
        "理論上，若是 agent 一直在進步，則所得到的 `avg_total_reward` 也會持續上升。\n",
        "若將其畫出來則結果如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZYOI8H10SHN",
        "outputId": "d1235139-aad3-43c2-88fd-bd115c39ac17"
      },
      "outputs": [],
      "source": [
        "plt.plot(avg_total_rewards)\n",
        "plt.title(\"Total Rewards\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.cla()\n",
        "env.render_all()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2HaGRVEYGQS"
      },
      "source": [
        "## 測試\n",
        "在這邊我們替換環境使用的資料日期區間，並使用讀取紀錄點的方式來執行測試。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcDcvWSc6etb",
        "outputId": "ab57e67a-6208-4fb8-93ca-df0767c0c675"
      },
      "outputs": [],
      "source": [
        "testenv = MyStocksEnv(df=STOCKS_TSMC, window_size=10, frame_bound=(2000, 2500))\n",
        "\n",
        "network = PolicyGradientNetwork(testenv.shape[1])\n",
        "test_agent = PolicyGradientAgent(network)\n",
        "\n",
        "checkpoint_path = './model.ckpt'\n",
        "\n",
        "test_agent.load_ckpt(checkpoint_path)\n",
        "test_agent.network.eval()  # 測試前先將 network 切換為 evaluation 模式\n",
        "\n",
        "observation = testenv.reset()\n",
        "while True:\n",
        "    action, _ = test_agent.sample(observation)\n",
        "    observation, reward, done, info = testenv.step(action)\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "plt.cla()\n",
        "testenv.render_all()\n",
        "plt.show()\n",
        "# green buy, red sell\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8 (main, Oct 21 2022, 22:22:30) [Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "017bb6875a794fb5be36a5edc9edb03d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28658a1c9e724b9a85cc2878faf2d8e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4de76787a98948b89f9ee9b311a42253": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64cb3c25c9a548789b78192cc5e8f214": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee8a2b45b6ba48db9ccdd742c0e74fda",
            "max": 400,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a2e05b9960dc4800974704b05adf997e",
            "value": 400
          }
        },
        "752a369955b54de9b714a2e54ea33824": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28658a1c9e724b9a85cc2878faf2d8e2",
            "placeholder": "​",
            "style": "IPY_MODEL_017bb6875a794fb5be36a5edc9edb03d",
            "value": "Average Reward:  34.50, Final Reward:  38.00, Final Profit:  1.17: 100%"
          }
        },
        "a2e05b9960dc4800974704b05adf997e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "abe58da207ce4efc9649a8e7fc717682": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1ec572744fa41379b26e872c4c9631a",
            "placeholder": "​",
            "style": "IPY_MODEL_4de76787a98948b89f9ee9b311a42253",
            "value": " 400/400 [13:13&lt;00:00,  1.94s/it]"
          }
        },
        "b1723c3ecd394c4ebca7ab2063af2d89": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1ec572744fa41379b26e872c4c9631a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da5a2579b8784924b4733cc578d2a309": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_752a369955b54de9b714a2e54ea33824",
              "IPY_MODEL_64cb3c25c9a548789b78192cc5e8f214",
              "IPY_MODEL_abe58da207ce4efc9649a8e7fc717682"
            ],
            "layout": "IPY_MODEL_b1723c3ecd394c4ebca7ab2063af2d89"
          }
        },
        "ee8a2b45b6ba48db9ccdd742c0e74fda": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
